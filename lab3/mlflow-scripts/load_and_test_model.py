import os
import warnings
import urllib3
from constants import MLFLOW_TRACKING_URI, MLFLOW_HOST_HEADER
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

print("="*80)
print("4. –ó–ê–ì–†–£–ó–ö–ê –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ò–ó MLFLOW REGISTRY")
print("="*80)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')


os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = "true"

if 'MLFLOW_TRACKING_SERVER_CERT_PATH' in os.environ:
    del os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH']

print(f"\n[–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø]")
print(f"  - Tracking URI: {MLFLOW_TRACKING_URI}")
print(f"  - Host Header: {MLFLOW_HOST_HEADER}")


import requests

original_session_init = requests.Session.__init__

def patched_session_init(self, *args, **kwargs):
    original_session_init(self, *args, **kwargs)
    self.headers.update({'Host': MLFLOW_HOST_HEADER})

requests.Session.__init__ = patched_session_init


import mlflow
import mlflow.pyfunc

mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)


print("\n" + "="*80)
print("–ü–û–î–ì–û–¢–û–í–ö–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•")
print("="*80)

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\n‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
print(f"  - Training set: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
print(f"  - Test set: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
print(f"  - Features: {X_train.shape[1]}")


print("\n" + "="*80)
print("–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –ò–ó REGISTRY")
print("="*80)

models_to_test = [
    "iris-logistic-regression",
    "iris-random-forest"
]

results = {}

for model_name in models_to_test:
    print(f"\n[{model_name}]")

    try:
        model_uri = f"models:/{model_name}/Production"
        model = mlflow.pyfunc.load_model(model_uri)

        print(f"  ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        print(f"    - URI: {model_uri}")


        print(f"\n  [–¢–ï–°–¢ 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ]")

        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        print(f"    - Accuracy:  {accuracy:.4f}")
        print(f"    - Precision: {precision:.4f}")
        print(f"    - Recall:    {recall:.4f}")
        print(f"    - F1-score:  {f1:.4f}")

        results[model_name] = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }


        print(f"\n  [–¢–ï–°–¢ 2: –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫]")

        cm = confusion_matrix(y_test, y_pred)
        print(f"\n    –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (3x3 –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤):")
        print(f"    –°—Ç—Ä–æ–∫–∏ = –∏—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã, —Å—Ç–æ–ª–±—Ü—ã = –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ")

        for i, row in enumerate(cm):
            print(f"    –ö–ª–∞—Å—Å {i}: {row}")


        print(f"\n  [–¢–ï–°–¢ 3: –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º]")

        report = classification_report(y_test, y_pred, target_names=iris.target_names)
        print(f"\n{report}")


        print(f"  [–¢–ï–°–¢ 4: –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–ø–µ—Ä–≤—ã–µ 10)]")

        for i in range(min(10, len(X_test))):
            pred_class = y_pred[i]
            true_class = y_test[i]
            pred_name = iris.target_names[pred_class]
            true_name = iris.target_names[true_class]

            status = "‚úì" if pred_class == true_class else "‚úó"
            print(f"    {status} –û–±—Ä–∞–∑–µ—Ü {i+1}: –ø—Ä–µ–¥—Å–∫–∞–∑ = {pred_name:15} | —Ñ–∞–∫—Ç = {true_name}")


        print(f"\n  [–¢–ï–°–¢ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è]")

        y_pred_train = model.predict(X_train)
        accuracy_train = accuracy_score(y_train, y_pred_train)

        print(f"    - Accuracy –Ω–∞ train: {accuracy_train:.4f}")
        print(f"    - Accuracy –Ω–∞ test:  {accuracy:.4f}")

        overfit_ratio = (accuracy_train - accuracy) / accuracy * 100
        print(f"    - –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: {overfit_ratio:+.2f}%")

        if overfit_ratio > 10:
            print(f"      ‚ö† –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!")
        else:
            print(f"      ‚úì –ú–æ–¥–µ–ª—å –≤ –Ω–æ—Ä–º–µ")

    except Exception as e:
        print(f"  ‚úó –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()


print("\n" + "="*80)
print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
print("="*80)

if len(results) > 1:
    print("\n[–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫]")

    for model_name, metrics in results.items():
        print(f"\n  {model_name}:")
        for metric, value in metrics.items():
            print(f"    - {metric}: {value:.4f}")

    best_model = max(results.items(), key=lambda x: x[1]["accuracy"])
    print(f"\n  üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model[0]} (accuracy: {best_model[1]['accuracy']:.4f})")
