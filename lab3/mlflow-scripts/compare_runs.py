import os
import warnings
import urllib3

print("="*80)
print("5. –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–ü–£–°–ö–û–í MLFLOW")
print("="*80)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')


from constants import MLFLOW_TRACKING_URI, MLFLOW_HOST_HEADER, EXPERIMENT_NAME, METRICS

os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = "true"

if 'MLFLOW_TRACKING_SERVER_CERT_PATH' in os.environ:
    del os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH']

print(f"\n[–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø]")
print(f"  - Tracking URI: {MLFLOW_TRACKING_URI}")
print(f"  - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {EXPERIMENT_NAME}")


import requests

original_session_init = requests.Session.__init__

def patched_session_init(self, *args, **kwargs):
    original_session_init(self, *args, **kwargs)
    self.headers.update({'Host': MLFLOW_HOST_HEADER})

requests.Session.__init__ = patched_session_init


import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd

mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)
client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)


print("\n" + "="*80)
print("–ü–û–õ–£–ß–ï–ù–ò–ï –í–°–ï–• RUNS")
print("="*80)

try:
    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)
    if not experiment:
        print(f"‚úó –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {EXPERIMENT_NAME}")
        exit(1)

    print(f"\n‚úì –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω: {experiment.name} (ID: {experiment.experiment_id})")

    runs = client.search_runs(experiment_ids=[experiment.experiment_id])
    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ runs: {len(runs)}")

except Exception as e:
    print(f"\n‚úó –û–®–ò–ë–ö–ê: {e}")
    import traceback
    traceback.print_exc()
    exit(1)


print("\n" + "="*80)
print("–ê–ù–ê–õ–ò–ó –ú–ï–¢–†–ò–ö")
print("="*80)

run_data = []

def col_name(metric_key: str) -> str:
    return "F1-Score" if metric_key == "f1_score" else metric_key.capitalize()

for run in runs:

    params = run.data.params
    metrics = run.data.metrics

    run_info = {
        "Run Name": run.info.run_name,
        "Run ID": run.info.run_id[:8] + "...",  # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π ID
        "Status": run.info.status,
    }

    for m in METRICS:
        run_info[col_name(m)] = metrics.get(m, 0)

    run_data.append(run_info)


if run_data:
    df = pd.DataFrame(run_data)

    print("\n[–¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö runs]")
    print("\n" + df.to_string(index=False))


    print("\n" + "="*80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*80)

    metrics_cols = [col_name(m) for m in METRICS]

    for metric in metrics_cols:
        values = df[metric]
        print(f"\n[{metric}]")
        print(f"  - –ú–∏–Ω–∏–º—É–º: {values.min():.4f}")
        print(f"  - –ú–∞–∫—Å–∏–º—É–º: {values.max():.4f}")
        print(f"  - –°—Ä–µ–¥–Ω–µ–µ:  {values.mean():.4f}")
        print(f"  - –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {values.std():.4f}")


    print("\n" + "="*80)
    print("–õ–£–ß–®–ò–ï RUNS")
    print("="*80)

    best_accuracy = df.loc[df["Accuracy"].idxmax()]
    best_f1 = df.loc[df["F1-Score"].idxmax()]

    print(f"\nüèÜ –õ—É—á—à–∏–π –ø–æ Accuracy:")
    print(f"  - Model: {best_accuracy['Run Name']}")
    print(f"  - Accuracy: {best_accuracy['Accuracy']:.4f}")

    print(f"\nüèÜ –õ—É—á—à–∏–π –ø–æ F1-Score:")
    print(f"  - Model: {best_f1['Run Name']}")
    print(f"  - F1-Score: {best_f1['F1-Score']:.4f}")


    print("\n" + "="*80)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –ü–û –¢–ò–ü–ê–ú –ú–û–î–ï–õ–ï–ô")
    print("="*80)

    lr_runs = df[df["Run Name"].str.contains("Logistic", case=False)]
    rf_runs = df[df["Run Name"].str.contains("Random", case=False)]

    if len(lr_runs) > 0:
        print(f"\n[Logistic Regression] ({len(lr_runs)} –∑–∞–ø—É—Å–∫–æ–≤)")
        print(f"  - –°—Ä–µ–¥–Ω–∏–π Accuracy: {lr_runs['Accuracy'].mean():.4f}")
        print(f"  - –°—Ä–µ–¥–Ω–∏–π F1-Score: {lr_runs['F1-Score'].mean():.4f}")

    if len(rf_runs) > 0:
        print(f"\n[Random Forest] ({len(rf_runs)} –∑–∞–ø—É—Å–∫–æ–≤)")
        print(f"  - –°—Ä–µ–¥–Ω–∏–π Accuracy: {rf_runs['Accuracy'].mean():.4f}")
        print(f"  - –°—Ä–µ–¥–Ω–∏–π F1-Score: {rf_runs['F1-Score'].mean():.4f}")

    if len(lr_runs) > 0 and len(rf_runs) > 0:
        lr_acc = lr_runs['Accuracy'].mean()
        rf_acc = rf_runs['Accuracy'].mean()

        print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:")
        if lr_acc > rf_acc:
            print(f"  Logistic Regression –ª—É—á—à–µ –Ω–∞ {(lr_acc - rf_acc)*100:.2f}%")
        elif rf_acc > lr_acc:
            print(f"  Random Forest –ª—É—á—à–µ –Ω–∞ {(rf_acc - lr_acc)*100:.2f}%")
        else:
            print(f"  –ú–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω—ã")
